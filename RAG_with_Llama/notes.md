# What a â€œLarge Language Modelâ€ Really Is

A model like **LLaMA / Mistral / Phi** is just a large collection of numbers (parameters).

Example: **LLaMA-2-7B**
- ~7 billion parameters
- Stored as tensors
- Each parameter occupies memory depending on precision

## Memory usage (approx)

| Precision | Memory |
|--------|--------|
| FP32 | ~28 GB |
| FP16 / BF16 | ~14 GB |
| INT8 | ~7 GB |
| INT4 | ~3.5 GB |

Extra memory is needed for:
- Attention KV cache
- Activations
- Optimizer states (training only)


# Inference vs Fine-Tuning

## Inference
- Read-only model
- Forward pass only
- Used for chat / RAG / generation

Memory needs:
- Model weights
- KV cache

## Fine-Tuning
- Forward + backward pass
- Updates parameters

Memory needs:
- Weights
- Gradients
- Optimizer states
- Activations

> Fine-tuning requires **4â€“8Ã— more memory** than inference.


# Where the Model Lives

All usage methods fall into one of these:

1. **Remote APIs** (OpenAI, Gemini, Claude)
2. **Local inference** (Hugging Face, Ollama)
3. **Managed cloud models** (HF Endpoints, SageMaker)

Each option trades:
- Control
- Cost
- Performance
- Privacy


# Fully Hosted APIs

Examples:
- OpenAI
- Google Gemini
- Anthropic Claude
- Groq
- Hugging Face Inference API

## What you control
- Prompt
- Temperature
- Max tokens

## What you do NOT control
- Model weights
- Architecture
- Tokenizer
- Training data

## Fine-tuning
âŒ No real weight updates  
âš ï¸ Sometimes instruction tuning only

Best for:
- Rapid prototyping
- Production apps


# Hugging Face Local Inference

## Non-quantized (FP16 / BF16)
- Best quality
- High VRAM usage
- Slow startup

## Quantized (INT8 / INT4)
- Lower memory
- Faster inference
- Small quality loss

### Popular formats
| Format | Used by |
|-----|------|
| INT8 | bitsandbytes |
| NF4 | QLoRA |
| GPTQ | ExLlama |
| AWQ | Fast kernels |
| GGUF | llama.cpp |


# Ollama & llama.cpp

Ollama uses **GGUF models** with custom inference engines.

## Why it works on laptops
- Heavy quantization
- Efficient KV cache
- CPU / GPU / Apple Metal support

## Capabilities
- Inference
- System prompts
- RAG

## Limitations
âŒ No training  
âŒ No LoRA fine-tuning  

Best for:
- Local experimentation
- Demos


# Full Fine-Tuning

Means updating **all parameters**.

## Hardware needs
| Model | GPUs |
|----|----|
| 7B | 4Ã—A100 |
| 13B | 8Ã—A100 |
| 70B | Cluster |

## Memory cost
Weights + Gradients + Optimizer â‰ˆ **6â€“8Ã— model size**

Used only by:
- Meta
- OpenAI
- Google


# Parameter-Efficient Fine-Tuning (PEFT)

Instead of training everything, train small components.

## LoRA
- Adds low-rank matrices
- Base model frozen
- Very memory efficient

## QLoRA (Industry Standard)
- Base model: 4-bit
- LoRA adapters: FP16

| Model | GPU |
|----|----|
| 7B | 16GB |
| 13B | 24GB |

Quality â‰ˆ **98% of full fine-tuning**


# Other PEFT Methods

| Method | Idea |
|----|----|
| Prefix Tuning | Virtual tokens |
| Prompt Tuning | Soft prompts |
| IAÂ³ | Activation scaling |
| Adapters | Insert trainable layers |

LoRA dominates because:
- Simple
- Mergeable
- Framework-agnostic


# Serving Your Own Model

## Popular inference engines
| Engine | Use |
|----|----|
| vLLM | High throughput |
| TGI | HF official |
| llama.cpp | CPU |
| Ollama | Local dev |
| ExLlama | Fast GPTQ |

## Architecture
Client â†’ API â†’ Inference Engine â†’ GPU


# RAG vs Fine-Tuning

## Why RAG dominates production
- No retraining
- Cheap
- Always fresh data
- Debuggable

## RAG Flow
Query â†’ Retrieve docs â†’ Inject context â†’ Generate answer

> 80â€“90% of real systems use **RAG**, not fine-tuning


# Hybrid & Advanced Approaches

## RAG + LoRA
- LoRA for style/behavior
- RAG for facts

## Other techniques
- Mixture of Experts (Mixtral)
- Speculative decoding (Groq)
- Distillation (large â†’ small)


# Mental Model Summary
```
Prompt
  â†“
LangChain
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama    â”‚ HF Local  â”‚ Hosted API â”‚
â”‚ (GGUF)    â”‚ (LoRA)    â”‚ (BlackBox) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
RAG â†’ default  
LoRA â†’ behavior  
Full fine-tuning â†’ rarely


1ï¸âƒ£ Memory Diagrams (Inference vs Full FT vs QLoRA)
ğŸ”¹ A) Inference (FP16, no training)
```
GPU VRAM
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Weights (FP16) ~14 GB  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ KV Cache (depends on tokens) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Activations (minimal)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âœ”ï¸ Fast
âœ”ï¸ Cheap
âŒ No learning

ğŸ”¹ B) Full Fine-Tuning (Why itâ€™s impossible for you)
```
GPU VRAM
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Weights (FP16)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gradients (FP16)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Optimizer States (Adam x2)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Activations (HUGE)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ’¥ 6â€“8Ã— model size
ğŸ’¥ 7B â‰ˆ 80â€“120 GB VRAM

âŒ Practically dead outside Big Tech

ğŸ”¹ C) QLoRA (What everyone actually uses)
```
GPU VRAM
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Base Model (4-bit NF4) ~3GB  â”‚  â† frozen
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LoRA Adapters (FP16) ~200MB â”‚  â† trainable
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Activations (paged)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âœ”ï¸ Fits on 16â€“24GB GPU
âœ”ï¸ Near full-FT quality
âœ”ï¸ Mergeable adapters

This is the industry standard

2ï¸âƒ£ Exact QLoRA Training Code (REAL, MODERN)

This is not tutorial code â€” this is what people actually run.

ğŸ”¹ Install
```python
pip install -U transformers accelerate peft bitsandbytes datasets trl
```
ğŸ”¹ Load Quantized Model
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto",
)

tokenizer = AutoTokenizer.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    use_fast=True
)
tokenizer.pad_token = tokenizer.eos_token
```
ğŸ”¹ Attach LoRA (THIS is the magic)
```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

Output:

Trainable params: ~5M
Total params: ~7B

ğŸ”¹ Train (TRL SFTTrainer)
```python
from trl import SFTTrainer
from datasets import load_dataset

dataset = load_dataset("json", data_files="train.jsonl")

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    max_seq_length=2048,
    packing=True,
    args={
        "per_device_train_batch_size": 2,
        "gradient_accumulation_steps": 8,
        "learning_rate": 2e-4,
        "fp16": False,
        "bf16": True,
        "num_train_epochs": 3,
        "logging_steps": 10,
        "save_steps": 500,
        "output_dir": "./qlora-out",
        "optim": "paged_adamw_8bit",
    },
)

trainer.train()
```

ğŸ”¥ This runs on a single 24GB GPU
```
3ï¸âƒ£ vLLM vs Ollama (Real Comparison)
Feature	vLLM	Ollama
Purpose	Production serving	Local dev
Throughput	â­â­â­â­â­	â­â­
GPU utilization	Excellent	Moderate
Batching	Continuous	Limited
RAG-ready	Yes	Yes
Fine-tuning	No	No
Model formats	HF	GGUF
Used by startups	YES	Rare
```
ğŸ”¹ Why vLLM is dominant
```
Requests
   â†“
Continuous Batching
   â†“
PagedAttention
   â†“
GPU stays 90â€“95% busy
```

Ollama:
```
Request â†’ Generate â†’ Idle â†’ Next
```

âœ”ï¸ Ollama = laptop tool
âœ”ï¸ vLLM = revenue-generating infra

4ï¸âƒ£ Why Fine-Tuning FAILS for RAG Use Cases
âŒ Myth

â€œIf I fine-tune the model on my documents, I donâ€™t need RAGâ€

Reality:
## ğŸ”¹ 1) Hallucinations increase

Model generalizes, not retrieves.

Document says:
"Policy updated on March 2024"

Model learns:
"Policies are usually updated annually"


âŒ Wrong answer

## ğŸ”¹ 2) Knowledge freezes instantly
```
Fine-tuned today
â†“
Policy changes tomorrow
â†“
Model is already wrong
```

RAG = real-time
Fine-tuning = static

## ğŸ”¹ 3) Token inefficiency

Model stores facts in weights

Retrieval stores facts in documents

Weights â‰  database

## ğŸ”¹ When fine-tuning DOES help
```
Use case	Method
Tone	LoRA
Reasoning style	LoRA
Domain language	LoRA
Facts	âŒ RAG
```
Correct architecture:

Base LLM
 + LoRA (style)
 + RAG (facts)

# 5ï¸âƒ£ Real Infrastructure Used by Startups (No BS)
## ğŸ”¹ Typical Series A Stack
```
Client
  â†“
FastAPI / Node
  â†“
LangChain / Custom RAG
  â†“
Vector DB (Qdrant / Pinecone)
  â†“
vLLM Inference Server
  â†“
A10 / L40 / A100 GPU
```
## ğŸ”¹ Models in production
```
Use	Model
General chat	Llama 3 / Mistral
Coding	DeepSeek / CodeLlama
Cheap inference	Phi / Gemma
RAG	Any 7Bâ€“13B
```
## ğŸ”¹ Cost reality (monthly)
```
Setup	Cost
OpenAI API	$$$
Self-hosted A10	~$800
Self-hosted L40	~$2,000
A100	$$$$

Startups move off APIs once:

Latency matters

Token cost explodes

Data privacy required
```
# 6ï¸âƒ£ Final Mental Model (Burn This In)
```
âŒ Fine-tune to learn facts
âœ… RAG to retrieve facts

âŒ Ollama for production
âœ… vLLM for production

âŒ Full fine-tuning
âœ… QLoRA

âŒ Store docs in weights
âœ… Store docs in vector DB
```


# 2ï¸âƒ£ QLoRA Training â€” Exact Production-Grade Code

## ğŸ”¹ Install
pip install -U transformers accelerate peft bitsandbytes datasets trl

---

## ğŸ”¹ Load Quantized Base Model
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto",
)

tokenizer = AutoTokenizer.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    use_fast=True
)
tokenizer.pad_token = tokenizer.eos_token
```


# 2ï¸âƒ£ QLoRA Training â€” Exact Production-Grade Code

## ğŸ”¹ Install
```
pip install -U transformers accelerate peft bitsandbytes datasets trl
```
---

## ğŸ”¹ Attach LoRA Adapters
```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```
---

## ğŸ”¹ Train (TRL SFTTrainer)

```python
from trl import SFTTrainer
from datasets import load_dataset

dataset = load_dataset("json", data_files="train.jsonl")

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    max_seq_length=2048,
    packing=True,
    args={
        "per_device_train_batch_size": 2,
        "gradient_accumulation_steps": 8,
        "learning_rate": 2e-4,
        "bf16": True,
        "num_train_epochs": 3,
        "logging_steps": 10,
        "save_steps": 500,
        "output_dir": "./qlora-out",
        "optim": "paged_adamw_8bit",
    },
)

trainer.train()
```

---


# 3ï¸âƒ£ vLLM vs Ollama â€” Real-World Comparison

| Feature | vLLM | Ollama |
|------|------|-------|
| Purpose | Production serving | Local dev |
| Throughput | â­â­â­â­â­ | â­â­ |
| GPU utilization | Excellent | Moderate |
| Batching | Continuous | Limited |
| RAG-ready | Yes | Yes |
| Fine-tuning | No | No |
| Model formats | HF | GGUF |
| Used by startups | YES | Rare |

---

## ğŸ”¹ Why vLLM Dominates

Requests  
â†“  
Continuous batching  
â†“  
PagedAttention  
â†“  
GPU stays ~90â€“95% busy  

---

## ğŸ”¹ Ollama Execution Model

Request â†’ Generate â†’ Idle â†’ Next  

âœ”ï¸ Ollama = laptop experimentation  
âœ”ï¸ vLLM = production revenue engine  


# 4ï¸âƒ£ Why Fine-Tuning Fails for RAG Use Cases

## âŒ Myth
"If I fine-tune on my documents, I donâ€™t need RAG"

---

## ğŸ”¹ Reality

### 1) Hallucinations Increase
Documents:
"Policy updated March 2024"

Model learns:
"Policies usually update annually" âŒ

---

### 2) Knowledge Freezes
Fine-tune today  
â†“  
Policy changes tomorrow  
â†“  
Model is instantly wrong  

---

### 3) Token Inefficiency
- Weights â‰  database  
- Retrieval â‰  memorization  

---

## ğŸ”¹ When Fine-Tuning Actually Helps

| Use Case | Method |
|-------|-------|
| Tone | LoRA |
| Reasoning style | LoRA |
| Domain phrasing | LoRA |
| Facts | âŒ RAG |

---

## âœ… Correct Architecture

Base LLM  
+ LoRA (behavior/style)  
+ RAG (facts & freshness)  

---

# 5ï¸âƒ£ Real Infrastructure Used by Startups

## ğŸ”¹ Typical Series-A Stack

Client  
â†“  
FastAPI / Node  
â†“  
LangChain / Custom RAG  
â†“  
Vector DB (Qdrant / Pinecone)  
â†“  
vLLM Inference Server  
â†“  
A10 / L40 / A100 GPU  

---

## ğŸ”¹ Models in Production

| Use | Model |
|----|------|
| Chat | Llama 3 / Mistral |
| Coding | DeepSeek / CodeLlama |
| Cheap inference | Phi / Gemma |
| RAG | Any 7Bâ€“13B |

---

## ğŸ”¹ Monthly Cost Reality

| Setup | Cost |
|----|-----|
| OpenAI API | $$$ |
| A10 (self-hosted) | ~$800 |
| L40 | ~$2,000 |
| A100 | $$$$ |

Teams move off APIs when:
- Latency matters  
- Token cost explodes  
- Privacy is required  


# 6ï¸âƒ£ Final Mental Model (Non-Negotiable)

âŒ Fine-tune to store facts  
âœ… Use RAG to retrieve facts  

âŒ Ollama for production  
âœ… vLLM for production  

âŒ Full fine-tuning  
âœ… QLoRA  

âŒ Knowledge in weights  
âœ… Knowledge in vector DB  

**RAG is default.  
Fine-tuning is optional.  
Instruction models are the base.**


